{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5SRmxdiewyLt",
   "metadata": {
    "id": "5SRmxdiewyLt"
   },
   "source": [
    "# USF GenAI Lab 1 ‚Äî **Admissions Chatbot for USF**\n",
    "**Instructor:** Dr. Alla Abdella  \n",
    "**Course:** EEL 6935 & EEL 4935 ‚Äî Advanced Generative AI Development\n",
    "**Department:** Electrical Engineering, University of South Florida (USF)\n",
    "\n",
    "\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will **build an intelligent USF Admissions Chatbot** that helps prospective students learn about the University of South Florida by:\n",
    "- Scraping and processing **public USF web content**.\n",
    "- Prompting local LLMs using **Ollama**.\n",
    "- Evaluating model responses using structured techniques.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What You‚Äôll Do\n",
    "- üîπ Replace each code cell‚Äôs **TODO scaffold** with your implementation.\n",
    "- üîπ Write **clean, documented, PEP8-compliant code**.\n",
    "- üîπ Cite all external sources used in your work.\n",
    "- üîπ Remove any `NotImplementedError` lines after completing tasks.\n",
    "\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Collect & preprocess** public USF information responsibly.\n",
    "2. **Engineer prompts** that integrate retrieved context.\n",
    "3. **Query local LLMs** using **Ollama** and **swap between models** for experiments.\n",
    "4. **Compare models** on:\n",
    "   - Response quality  \n",
    "   - Response length  \n",
    "   - Execution speed  \n",
    "5. **Evaluate answers** using:\n",
    "   - A structured **rubric**\n",
    "   - An **LLM-as-judge** evaluation approach  \n",
    "6. **Generate citations** or extract **references** for factual claims.\n",
    "\n",
    "\n",
    "\n",
    "##  Prerequisites\n",
    "> Before starting, ensure:\n",
    "- **Ollama** is installed and running locally:  \n",
    "  `http://localhost:11434`\n",
    "- The models you plan to test are downloaded:  \n",
    "  ```bash\n",
    "  ollama pull modle_name:model_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97e072e",
   "metadata": {
    "id": "a97e072e"
   },
   "source": [
    "# What to Submit\n",
    "\n",
    "Please include the following in your final submission:\n",
    "\n",
    "1. **Completed Notebook** ‚Äî All **TODOs** must be addressed.\n",
    "2. **Error-Free Execution** ‚Äî Ensure the notebook runs **from start to finish** without issues.\n",
    "3. **Final Comparison Table** ‚Äî Submit a CSV file.\n",
    "3. **Summary of Your Approach** ‚Äî Briefly explain your methodology and any **assumptions made**.\n",
    "4. **Insights on the following**\n",
    "   - Which **model performed best** and why.\n",
    "   - Where **citations succeeded or failed**.\n",
    "   - How you would **improve grounding and evaluation** in the future.\n",
    "\n",
    "\n",
    "##  Academic Integrity & Ethics\n",
    "- Use **only publicly accessible USF pages**.\n",
    "- **Do NOT** bypass authentication, scrape personal data, or overload servers.\n",
    "- If you change URLs, **limit changes to ‚â§ 3** and keep requests minimal.\n",
    "- Always **cite the exact pages** used when making factual claims.\n",
    "\n",
    "\n",
    "## Extensions *(Optional)*\n",
    "Push your analysis further by trying one or more of these:\n",
    "\n",
    "- Add **another USF page** to your scraping and re-evaluate the results.\n",
    "- Swap in a **bigger or smarter local model** and compare performance.\n",
    "- Use a **separate LLM as the judge** to evaluate responses independently.\n",
    "\n",
    "\n",
    "> **Tip:** Going beyond the base requirements can strengthen your understanding of **LLM evaluation techniques** and **prompt engineering strategies**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3pBA7GdZwyLu",
   "metadata": {
    "id": "3pBA7GdZwyLu"
   },
   "source": [
    "## 0) Setup & Imports\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "WvOVR7HBwyLv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WvOVR7HBwyLv",
    "outputId": "a69a10be-aab4-4678-af4b-5666ef6bc27b"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Lb_dJV2wyLw",
   "metadata": {
    "id": "5Lb_dJV2wyLw"
   },
   "source": [
    "## 1) Configuration\n",
    "Change the **models** list to compare different local models. All requests go to Ollama's `/api/generate` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "IYhmbsbwwyLw",
   "metadata": {
    "id": "IYhmbsbwwyLw"
   },
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    \"llama3.2:3b\",\n",
    "    \"mistral:7b\",\n",
    "    \"llama3.1:8b\"\n",
    "]\n",
    "\n",
    "# USF pages to scrape (keeping it minimal and respectful)\n",
    "USF_URLS = [\n",
    "    \"https://www.usf.edu/admissions/freshmen/admission-information/academic-requirements.aspx\",\n",
    "    \"https://www.usf.edu/about-usf/\",\n",
    "    \"https://www.usf.edu/facilities/service-center/index.aspx\"\n",
    "]\n",
    "\n",
    "# Configuration constants\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "DEFAULT_TEMPERATURE = 0\n",
    "MAX_CONTEXT_CHARS = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OQUvc1rvwyLw",
   "metadata": {
    "id": "OQUvc1rvwyLw"
   },
   "source": [
    "## 2) Responsible Scraper\n",
    "We‚Äôll fetch a few USF pages, strip boilerplate, and keep clean text for prompting. **Do not** overload servers; keep requests minimal and cache results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "YkN4Nk-bwyLw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkN4Nk-bwyLw",
    "outputId": "37083df6-6734-430c-e0d2-9163268c18d2"
   },
   "outputs": [],
   "source": [
    "def scrape_usf_pages(urls: List[str]):\n",
    "    \"\"\"Scrape main content from a list of USF web pages.\n",
    "\n",
    "    Args:\n",
    "        urls: List of URLs to scrape\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping URLs to their scraped content and metadata\n",
    "    \"\"\"\n",
    "    pages = {}\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "\n",
    "            # Add delay to be respectful to servers\n",
    "            time.sleep(1)\n",
    "\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Remove scripts, styles, navigation, and footer elements\n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                element.decompose()\n",
    "\n",
    "            # Try to find main content area\n",
    "            main_content = soup.find('main') or soup.find('div', class_=re.compile(r'content|main'))\n",
    "            if not main_content:\n",
    "                main_content = soup.find('body')\n",
    "\n",
    "            # Extract clean text\n",
    "            if main_content:\n",
    "                text = main_content.get_text(separator=' ', strip=True)\n",
    "                # Clean up whitespace\n",
    "                text = re.sub(r'\\s+', ' ', text)\n",
    "                text = text.strip()\n",
    "            else:\n",
    "                text = \"\"\n",
    "\n",
    "            pages[url] = {\n",
    "                'content': text,\n",
    "                'title': soup.title.string if soup.title else url,\n",
    "                'length': len(text)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            pages[url] = {\n",
    "                'content': f\"Error scraping content from {url}\",\n",
    "                'title': url,\n",
    "                'length': 0\n",
    "            }\n",
    "\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fMWJJouwyLx",
   "metadata": {
    "id": "2fMWJJouwyLx"
   },
   "source": [
    "## 3) System Prompt & Model Client\n",
    "We‚Äôll use the instructor‚Äôs system prompt and a thin client for streaming responses from Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gBihRb1lwyLx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gBihRb1lwyLx",
    "outputId": "1bfd3df8-6bc8-4bcf-d4fa-2a23a1b375a9"
   },
   "outputs": [],
   "source": [
    "class USFUniversity:\n",
    "    \"\"\"Client for interacting with Ollama-hosted LLMs for USF admissions chatbot.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = \"llama3.2:3b\", temperature: float = DEFAULT_TEMPERATURE):\n",
    "        \"\"\"Initialize the USFUniversity class.\n",
    "\n",
    "        Args:\n",
    "            model: Name of the Ollama model to use\n",
    "            temperature: Sampling temperature for generation\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.base_url = OLLAMA_BASE_URL\n",
    "\n",
    "        # System prompt for USF admissions chatbot\n",
    "        self.system_prompt = \"\"\"You are a helpful and knowledgeable USF (University of South Florida) admissions assistant\n",
    "        Your job is to:\n",
    "        1. Answer the students questions about USF using **only the information provided in the context above**\n",
    "        2. Write confidently and naturally, **as if you are an expert at USF**\n",
    "        3. Use **plain bullet points (‚Ä¢) or (-)** for all items ‚Äî do not use `+` or nested bullets\n",
    "        4. **Include citation numbers like [1], [2]** only after facts directly supported by the context\n",
    "        5. Include a **\"References\" section** with only the **used** citations in the response mapped to URLs.\n",
    "        6. If a question cannot be answered from the context, say so clearly (e.g., \"That information is not available in the current context.\")\n",
    "        7. Never refer to \"the context\" or \"the text above\" ‚Äî just present the information directly, as if it's from your own knowledge backed by citations\n",
    "        8. Avoid filler like \"According to the context provided\" or \"Based on the information above\".\n",
    "        9. Be concise but comprehensive.\"\"\"\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        \"\"\"Generate text based on the provided prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt: User prompt to generate response for\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing response, timing, and metadata\n",
    "        \"\"\"\n",
    "        full_prompt = f\"{self.system_prompt}\\n\\nUser: {prompt}\\n\\nAssistant:\"\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": full_prompt,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.base_url,\n",
    "                json=payload\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            result = response.json()\n",
    "            end_time = time.time()\n",
    "\n",
    "            return {\n",
    "                'response': result.get('response', ''),\n",
    "                'model': self.model,\n",
    "                'latency_seconds': end_time - start_time,\n",
    "                'success': True\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            end_time = time.time()\n",
    "            return {\n",
    "                'response': f\"Error generating response: {str(e)}\",\n",
    "                'model': self.model,\n",
    "                'latency_seconds': end_time - start_time,\n",
    "                'success': False\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2GLTRKeUwyLx",
   "metadata": {
    "id": "2GLTRKeUwyLx"
   },
   "source": [
    "## 4) Build the Context Prompt from Scraped Pages\n",
    "Create a compact **context block** with key facts from the scraped pages. Keep it under ~2‚Äì3k characters to avoid overlong prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sgM-qmxXwyLx",
   "metadata": {
    "id": "sgM-qmxXwyLx"
   },
   "outputs": [],
   "source": [
    "def build_context_block(pages, max_chars = MAX_CONTEXT_CHARS):\n",
    "    \"\"\"Build a context block from the main content of the provided pages.\n",
    "    \n",
    "    Args:\n",
    "        pages: Dictionary of scraped page data\n",
    "        max_chars: Maximum characters to include in context\n",
    "        \n",
    "    Returns:\n",
    "        Formatted context string with source references\n",
    "    \"\"\"\n",
    "    per_page_context_length = int(max_chars/len(pages))  # Limit per page to save space\n",
    "    context = \"USF INFORMATION CONTEXT\\n\"\n",
    "    \n",
    "    for i, (url, page_data) in enumerate(pages.items(), 1):\n",
    "        # Add source reference\n",
    "        context += f\"[{i}] Source: {page_data['title']} ({url})\\n\"\n",
    "        \n",
    "        # Add content (truncated if needed)\n",
    "        content = page_data['content']\n",
    "        if len(content) > per_page_context_length: \n",
    "            mid_content = len(content)/2\n",
    "            start_content = int(mid_content-per_page_context_length/2)\n",
    "            end_content = int(mid_content+per_page_context_length/2)\n",
    "            content = content[start_content:end_content] + \"...\"\n",
    "        \n",
    "        context += f\"Content: {content}\\n\"\n",
    "    \n",
    "    context += \"END CONTEXT\\n\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bBXCNyTwyLx",
   "metadata": {
    "id": "8bBXCNyTwyLx"
   },
   "source": [
    "## 5) Prompt Template (You will edit this)\n",
    "Write a task asking the assistant to summarize USF research strengths **with citations** pointing to the scraped sources.\n",
    "\n",
    "**Your Task:**\n",
    "1. Edit the `USER_TASK` below to ask a precise question.\n",
    "2. Keep: request for concise bullets + **cite specific URLs** from the context.\n",
    "3. Avoid claims not grounded in the context block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "B8H7JG7TwyLy",
   "metadata": {
    "id": "B8H7JG7TwyLy"
   },
   "outputs": [],
   "source": [
    "def build_user_prompt(context_block, task):\n",
    "    \"\"\"Build the complete user prompt with context and task.\n",
    "    \n",
    "    Args:\n",
    "        context_block: Formatted context from scraped pages\n",
    "        task: Specific task/question for the model\n",
    "        \n",
    "    Returns:\n",
    "        Complete formatted prompt\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"{context_block}\n",
    "\n",
    "TASK: {task}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use ONLY the information provided in the USF Information Context above.\n",
    "- Format your response with clear bullet points.\n",
    "- Cite sources using [1], [2], etc. format\n",
    "- Include a \"References\" section at the end mapping numbers to URLs used in the response. Do not add references which are not used in the response. \n",
    "- If information is not available in the context, state that clearly.\n",
    "- Be concise but comprehensive.\n",
    "\n",
    "\n",
    "Your response:\"\"\"\n",
    "    \n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dkG9t3E_wyLy",
   "metadata": {
    "id": "dkG9t3E_wyLy"
   },
   "source": [
    "## 6) Run Multiple Models & Collect Results\n",
    "We‚Äôll loop through `MODELS`, ask the same question, and capture answer, tokens (approx.), and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tMKP0q4twyLy",
   "metadata": {
    "id": "tMKP0q4twyLy"
   },
   "outputs": [],
   "source": [
    "def approx_token_count(text):\n",
    "    return len(text) // 4\n",
    "\n",
    "def ask_models(models, context_block, task, temperature = DEFAULT_TEMPERATURE):\n",
    "    \"\"\"Ask the specified models to perform the given task using the provided context.\n",
    "    \n",
    "    Args:\n",
    "        models: List of model names to query\n",
    "        context_block: Context information from scraped pages\n",
    "        task: Task/question to ask models\n",
    "        temperature: Sampling temperature\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with results from all models\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    user_prompt = build_user_prompt(context_block, task)\n",
    "    \n",
    "    for model in models:        \n",
    "        client = USFUniversity(model=model, temperature=temperature)\n",
    "        result = client.generate(user_prompt)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        response_length = len(result['response'])\n",
    "        response_tokens = approx_token_count(result['response'])\n",
    "        \n",
    "        results.append({\n",
    "            'model': model,\n",
    "            'response': result['response'],\n",
    "            'latency_seconds': result['latency_seconds'],\n",
    "            'response_length_chars': response_length,\n",
    "            'response_tokens_approx': response_tokens,\n",
    "            'success': result['success']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62-TyNI8wyLy",
   "metadata": {
    "id": "62-TyNI8wyLy"
   },
   "source": [
    "## 7) Evaluation Rubric +  LLM‚Äëas‚ÄëJudge\n",
    "Score each answer on a 1‚Äì5 scale for:\n",
    "- **Groundedness** (stays within the provided context)\n",
    "- **Specificity** (concrete facts vs. vagueness)\n",
    "- **Citations** (uses and maps [1], [2] to URLs)\n",
    "- **Clarity** (readable bullets)\n",
    "-**LLM-as-Judge** you can ask a judge model to score using the same rubric. (By default, it uses the **same** model list‚Äîfeel free to pick a separate judge model.)\n",
    "Great paper to learn about LLM-as-Judge prompts is here: https://arxiv.org/pdf/2306.05685\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nlZOwT9TwyLy",
   "metadata": {
    "id": "nlZOwT9TwyLy"
   },
   "outputs": [],
   "source": [
    "def judge_answer(model, context_block, answer):\n",
    "    \"\"\"Judge the quality of the generated answer based on the context.\n",
    "    \n",
    "    Args:\n",
    "        model: Model name to use as judge\n",
    "        context_block: Original context provided\n",
    "        answer: Generated answer to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with scores for different criteria (1-5 scale)\n",
    "    \"\"\"\n",
    "    judge_prompt = f\"\"\"You are an expert evaluator. Please score the following answer on a scale of 1-5 for each criterion:\n",
    "\n",
    "CONTEXT PROVIDED:\n",
    "{context_block}\n",
    "\n",
    "ANSWER TO EVALUATE:\n",
    "{answer}\n",
    "\n",
    "Please rate the answer on these criteria (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent):\n",
    "\n",
    "1. GROUNDEDNESS: Does the answer stick to information in the provided context?\n",
    "2. SPECIFICITY: Are the facts concrete and specific rather than vague?\n",
    "3. CITATIONS: Are sources properly cited with [1], [2] format and References has only the cited urls?\n",
    "4. CLARITY: Is the answer well-organized and easy to read?\n",
    "\n",
    "Respond ONLY with four numbers separated by commas (e.g., \"4,3,5,4\"):\n",
    "\"\"\"\n",
    "        \n",
    "    try:\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": judge_prompt,\n",
    "            \"temperature\": DEFAULT_TEMPERATURE,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "                OLLAMA_BASE_URL,\n",
    "                json=payload\n",
    "                #timeout=60\n",
    "            )\n",
    "        response.raise_for_status()\n",
    "\n",
    "        result = response.json().get('response','')\n",
    "\n",
    "        # Correct parsing\n",
    "        score_text = result.strip()\n",
    "        scores = [int(x.strip()) for x in score_text.split(',')[:4]]\n",
    "\n",
    "        return {\n",
    "            'groundedness': scores[0] if len(scores) > 0 else 3,\n",
    "            'specificity': scores[1] if len(scores) > 1 else 3,\n",
    "            'citations': scores[2] if len(scores) > 2 else 3,\n",
    "            'clarity': scores[3] if len(scores) > 3 else 3\n",
    "        }\n",
    "\n",
    "    except:\n",
    "        # Default scores if evaluation fails\n",
    "        return {'groundedness': 3, 'specificity': 3, 'citations': 3, 'clarity': 3}\n",
    "\n",
    "def evaluate_all(results_df, judge_model):\n",
    "    \"\"\"Evaluate all results in the DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with model results\n",
    "        judge_model: Model to use for evaluation (defaults to first model)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with evaluation scores added\n",
    "    \"\"\"\n",
    "    if judge_model is None:\n",
    "        judge_model = results_df.iloc[0]['model']\n",
    "        \n",
    "    judged_scores = []\n",
    "    \n",
    "    for idx, row in results_df.iterrows():\n",
    "        context = row.get(\"context\", \"\")\n",
    "        answer = row.get(\"response\", \"\")\n",
    "\n",
    "        scores = judge_answer(judge_model, context, answer)\n",
    "        \n",
    "        judged_scores.append(scores)\n",
    "        time.sleep(1)\n",
    "\n",
    "    results_df[\"llm_judged_scores\"] = judged_scores\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S8xYGcirwyLy",
   "metadata": {
    "id": "S8xYGcirwyLy"
   },
   "source": [
    "## 8) Citations Function\n",
    "This function attempts to **extract citations** in the `[n]` pattern from an answer and map them to URLs found in the **References** section. If none found, it heuristically matches page URLs to the answer via fuzzy keyword overlap (very naive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "jgOf7OFEwyLy",
   "metadata": {
    "id": "jgOf7OFEwyLy"
   },
   "outputs": [],
   "source": [
    "def extract_citations(answer):\n",
    "    \"\"\"Extract in-text citations in [n] format from the main body of the answer (ignores References section).\"\"\"\n",
    "\n",
    "    # Only keep content before the References section (case-insensitive)\n",
    "    main_text = re.split(r'\\b[Rr]eferences\\b', answer)[0]\n",
    "    \n",
    "    # Find all [n] patterns\n",
    "    citations = re.findall(r'\\[(\\d+)\\]', main_text)\n",
    "    \n",
    "    return [int(c) for c in citations]\n",
    "\n",
    "def citations_report(answer, pages):\n",
    "    \"\"\"Generate a report of citations for the given answer and the reference page data.\n",
    "    \n",
    "    Args:\n",
    "        answer: Model-generated answer that may contain [n] citations\n",
    "        pages: Scraped USF page content, dict with URL keys\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with citation stats\n",
    "    \"\"\"\n",
    "    # Extract only citations from the main body (not from the References section)\n",
    "    citations_found = extract_citations(answer)\n",
    "\n",
    "    # Check if a References section is present (case-insensitive)\n",
    "    has_references = bool(re.search(r'\\b[Rr]eferences\\b', answer))\n",
    "\n",
    "    # Map citations to URLs, assuming citations like [1] ‚Üí pages.keys()[0], etc.\n",
    "    urls = list(pages.keys())\n",
    "    citation_mapping = {}\n",
    "\n",
    "    for cite_num in citations_found:\n",
    "        if 1 <= cite_num <= len(urls):\n",
    "            citation_mapping[cite_num] = urls[cite_num - 1]\n",
    "\n",
    "    return {\n",
    "        'citations_found': citations_found,\n",
    "        'num_citations': len(citations_found),\n",
    "        'has_references_section': has_references\n",
    "        # 'citation_mapping': citation_mapping,\n",
    "        # 'properly_cited': len(citation_mapping) > 0 and has_references\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oTrHPK9qwyLy",
   "metadata": {
    "id": "oTrHPK9qwyLy"
   },
   "source": [
    "## 9) Comparison Table (Final)\n",
    "Join runtime stats and rubric scores into a single table and save to CSV for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "Ep_gcfTWwyLy",
   "metadata": {
    "id": "Ep_gcfTWwyLy"
   },
   "outputs": [],
   "source": [
    "def return_scores(model, input_prompt, generated_output):\n",
    "    \"\"\"Return relevance, coherence, and factual accuracy scores for the generated output.\"\"\"\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    # ---------- Relevance: TF-IDF cosine similarity ----------\n",
    "    try:\n",
    "        tfidf = TfidfVectorizer().fit_transform([input_prompt, generated_output])\n",
    "        relevance_score = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "    except:\n",
    "        relevance_score = 0.0  # fallback for empty or invalid input\n",
    "    scores['relevance'] = round(float(relevance_score), 3)\n",
    "    \n",
    "    # ---------- Coherence: based on structure ----------\n",
    "    sentences = re.split(r'[.!?]', generated_output)\n",
    "    sentences = [s.strip() for s in sentences if len(s.strip()) > 0]\n",
    "    avg_sentence_len = np.mean([len(s.split()) for s in sentences]) if sentences else 0\n",
    "    num_paragraphs = generated_output.count('\\n\\n') + 1\n",
    "\n",
    "    # Normalize: we assume ideal sentence length ~12 words, paragraph count ~2+\n",
    "    coherence_raw = min(1.0, (len(sentences)/3 + num_paragraphs/2 + avg_sentence_len/12) / 3)\n",
    "    scores['coherence'] = round(coherence_raw, 3)\n",
    "    \n",
    "    # ---------- Factual Accuracy: based on citations ----------\n",
    "    citations = extract_citations(generated_output)\n",
    "    num_citations = len(citations)\n",
    "\n",
    "    # Normalize: max 5 citations ‚Üí 1.0 score\n",
    "    factual_score = min(1.0, num_citations / 5.0)\n",
    "\n",
    "    # Add bonus if phrasing indicates attribution\n",
    "    if re.search(r'\\b(according to|based on|source|reference)\\b', generated_output.lower()):\n",
    "        factual_score += 0.1\n",
    "    scores['factual_accuracy'] = round(min(1.0, factual_score), 3)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce887deb",
   "metadata": {},
   "source": [
    "## Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "IjhqUo6YwyLz",
   "metadata": {
    "id": "IjhqUo6YwyLz"
   },
   "outputs": [],
   "source": [
    "def end_to_end_pipeline():\n",
    "    \"\"\"Run the entire end-to-end chatbot pipeline and validate results with a few prompts.\"\"\"\n",
    "    \n",
    "    # Step 1: Scrape USF pages\n",
    "    pages = scrape_usf_pages(USF_URLS)\n",
    "    \n",
    "    # Step 2: Build context\n",
    "    context_block = build_context_block(pages)\n",
    "    \n",
    "    # Step 3: Define test prompts\n",
    "    test_prompts = [\n",
    "        \"What are the admission requirements for undergraduate programs at USF?\",\n",
    "        \"Can you provide information about the campus facilities at USF?\",\n",
    "        \"What research opportunities are available for students at USF?\",\n",
    "        \"How does USF support student mental health and well-being?\",\n",
    "        \"What are the career services offered by USF to help students with job placements?\"\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Step 4: Test each prompt\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        try:\n",
    "            # Query models\n",
    "            results = ask_models(MODELS, context_block, prompt)\n",
    "            \n",
    "            # Add prompt info\n",
    "            results['prompt'] = prompt\n",
    "\n",
    "            # Context info\n",
    "            results['context'] = context_block\n",
    "            \n",
    "            # Quick evaluation\n",
    "            for idx, row in results.iterrows():\n",
    "                scores = return_scores(row['model'], prompt, row['response'])\n",
    "                for key, value in scores.items():\n",
    "                    results.loc[idx, key] = value\n",
    "                \n",
    "                # Citation analysis\n",
    "                cite_report = citations_report(row['response'], pages)\n",
    "                results.loc[idx, 'num_of_citations_found'] = cite_report['num_citations']\n",
    "            \n",
    "            all_results.append(results)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing prompt {i}: {str(e)}\")\n",
    "    \n",
    "    # Step 5: Combine and save results\n",
    "    if all_results:\n",
    "        final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Evaluate with LLaMA judge\n",
    "        final_results = evaluate_all(final_results, judge_model='llama3.1:8b')\n",
    "        \n",
    "        # Reorder columns: prompt before response\n",
    "        columns_order = ['model', 'prompt', 'response'] + [col for col in final_results.columns if col not in ['model', 'prompt', 'response']]\n",
    "        final_results = final_results[columns_order]\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = \"usf_chatbot_results_nishat.csv\"\n",
    "        final_results.to_csv(output_file, index=False)\n",
    "\n",
    "        \n",
    "# ==================== MAIN EXECUTION ====================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete pipeline\n",
    "    end_to_end_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57440a",
   "metadata": {},
   "source": [
    "## Report\n",
    "To build the full pipeline, I followed these main steps:\n",
    "1. Scraping: First, I scraped the content from a list of USF web pages that I selected.\n",
    "2. Context Construction: Since we had to stay under a 3000-character limit for the context block, I took about 1000 characters from the middle of each page. I figured that the middle section might hold the most relevant info ‚Äî but I do realize this could miss important details at the top or bottom of the page.\n",
    "3. Prompting the Models: I created a complete prompt by combining: a system prompt,the context block, and a user question & instructions. I used this prompt to query each model and collected the responses.\n",
    "4. Evaluation: I logged each model's in response and necessary informations in the CSV file for evaluation. \n",
    "5. LLM as judge: Finally, I used a separate model as a judge to score all responses on:Groundedness, Specificity, Citation usage & Clarity\n",
    "\n",
    "**My insights:**\n",
    "\n",
    "Out of all the models tested, I found that Model llama3.1:8b gave the best overall performance. It was more consistent in grounding its answers in the context and was better at clarity and structure. It also handled citations more carefully, even though no model was perfect in that area. Also, it took the least time to generate response for all tasks. \n",
    "\n",
    "Some models correctly included citations when the answer clearly came from the context. There were still a bunch of issues:\n",
    "1. Models sometimes included citations that weren‚Äôt actually referenced in the answer.\n",
    "2. Sometimes, they gave accurate information but didn‚Äôt cite it at all.\n",
    "3. In a few cases, I noticed hallucinated citations ‚Äî numbers that didn‚Äôt refer to anything real in the context.\n",
    "\n",
    "To improve the pipeline, I may\n",
    "1. Try to include more relevant content per page (maybe by summarizing or ranking key sections instead of grabbing text from the middle or by increasing the character limit).\n",
    "2. Use a smarter method to extract context ‚Äî maybe something like keyphrase matching or a content ranker.\n",
    "\n",
    "To help with this assignment, I referred to Claude AI, ChatGPT, and a few YouTube videos ([1], [2], [3]).\n",
    "\n",
    "References:\n",
    "\n",
    "[1] https://www.youtube.com/watch?v=bargNl2WeN4\n",
    "\n",
    "[2] https://www.youtube.com/watch?v=xjA1HjvmoMY&t=369s\n",
    "\n",
    "[3] https://www.youtube.com/watch?v=UtSSMs6ObqY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f75900",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
